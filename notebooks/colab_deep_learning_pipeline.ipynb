{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b561a19c",
   "metadata": {},
   "source": [
    "## üîß Setup: Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a96d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available. Training will be slower on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add08d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch pandas numpy yfinance scikit-learn transformers matplotlib seaborn tqdm\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00476ce9",
   "metadata": {},
   "source": [
    "## üìÇ Option 1: Clone from GitHub (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ada97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your code is on GitHub, clone it here\n",
    "# !git clone https://github.com/YOUR_USERNAME/stock_price_prediction.git\n",
    "# %cd stock_price_prediction\n",
    "print(\"Skip this cell if uploading files manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f49e9",
   "metadata": {},
   "source": [
    "## üìÇ Option 2: Upload Project Files to Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c606c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload project files if not using GitHub\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs('src', exist_ok=True)\n",
    "os.makedirs('data/checkpoints', exist_ok=True)\n",
    "os.makedirs('data/raw/news_cache', exist_ok=True)\n",
    "\n",
    "print(\"Upload your Python files (model.py, train.py, etc.) when prompted...\")\n",
    "# Uncomment to upload:\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c8a28d",
   "metadata": {},
   "source": [
    "## üì¶ Define All Required Modules Inline (Standalone Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import os\n",
    "\n",
    "# Add your NewsAPI key here\n",
    "NEWS_API_KEY = \"c5f10cd6942f4917a04c5a8d41119d80\"  # Replace with your key\n",
    "HISTORY_DAYS = 5\n",
    "NEWSAPI_ENDPOINT = \"https://newsapi.org/v2/everything\"\n",
    "INCLUDE_GLOBAL_SENTIMENT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43903b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Gathering Module\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cache directory\n",
    "CACHE_DIR = Path(\"data/raw/news_cache\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_sentiment_pipeline = None\n",
    "\n",
    "def get_sentiment_pipeline():\n",
    "    global _sentiment_pipeline\n",
    "    if _sentiment_pipeline is None:\n",
    "        _sentiment_pipeline = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "            revision=\"714eb0f\"\n",
    "        )\n",
    "    return _sentiment_pipeline\n",
    "\n",
    "def fetch_newsapi_headlines(query: str, from_date: str, to_date: str, page_size: int = 20):\n",
    "    cache_file = CACHE_DIR / f\"{query}_{from_date}_{to_date}.json\"\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"from\": from_date,\n",
    "        \"to\": to_date,\n",
    "        \"language\": \"en\",\n",
    "        \"pageSize\": page_size,\n",
    "        \"sortBy\": \"publishedAt\",\n",
    "        \"apiKey\": NEWS_API_KEY,\n",
    "    }\n",
    "    resp = requests.get(NEWSAPI_ENDPOINT, params=params)\n",
    "    j = resp.json()\n",
    "    if j.get(\"status\") != \"ok\":\n",
    "        print(f\"[WARN] NewsAPI error for {query}: {j}\")\n",
    "        headlines = []\n",
    "    else:\n",
    "        headlines = [a.get(\"title\", \"\") for a in j.get(\"articles\", [])]\n",
    "    with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(headlines, f, ensure_ascii=False, indent=2)\n",
    "    return headlines\n",
    "\n",
    "def compute_sentiment_score(headlines):\n",
    "    if not headlines:\n",
    "        return 0.0\n",
    "    pipe = get_sentiment_pipeline()\n",
    "    scores = []\n",
    "    for h in headlines:\n",
    "        try:\n",
    "            result = pipe(h)[0]\n",
    "            label = result[\"label\"].lower()\n",
    "            score = result[\"score\"]\n",
    "            if \"pos\" in label:\n",
    "                scores.append(score)\n",
    "            elif \"neg\" in label:\n",
    "                scores.append(-score)\n",
    "            else:\n",
    "                scores.append(0.0)\n",
    "        except Exception:\n",
    "            scores.append(0.0)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "def gather_data(ticker: str, days_back=60):\n",
    "    \"\"\"Gather stock data with features\"\"\"\n",
    "    end = datetime.today()\n",
    "    start_stock = end - timedelta(days=days_back)\n",
    "    NEWS_DAYS = 20\n",
    "    start_news = end - timedelta(days=NEWS_DAYS)\n",
    "\n",
    "    df = yf.download(ticker, start=start_stock, end=end, progress=False, auto_adjust=False)\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No data found for {ticker}\")\n",
    "    vix = yf.download(\"^VIX\", start=start_stock, end=end, progress=False, auto_adjust=False)\n",
    "    df[\"vix_index\"] = vix[\"Close\"].reindex(df.index).ffill()\n",
    "\n",
    "    sentiments = []\n",
    "    for dt in df.index:\n",
    "        date_str = dt.strftime(\"%Y-%m-%d\")\n",
    "        if dt < start_news:\n",
    "            comp_score, global_score = 0.0, 0.0\n",
    "        else:\n",
    "            try:\n",
    "                company_news = fetch_newsapi_headlines(ticker, date_str, date_str)\n",
    "                comp_score = compute_sentiment_score(company_news)\n",
    "            except:\n",
    "                comp_score = 0.0\n",
    "            global_score = 0.0\n",
    "        sentiments.append((comp_score, global_score))\n",
    "    df[\"sentiment_comp\"] = [s[0] for s in sentiments]\n",
    "    df[\"sentiment_global\"] = [s[1] for s in sentiments]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    df[\"interest_rate\"] = 5.0 + np.random.normal(0, 0.1, len(df))\n",
    "    df[\"inflation_rate\"] = 2.5 + np.random.normal(0, 0.05, len(df))\n",
    "    df[\"gdp_growth\"] = 1.8 + np.random.normal(0, 0.03, len(df))\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(HISTORY_DAYS, len(df)-1):\n",
    "        window = df.iloc[i-HISTORY_DAYS:i][[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]].values.flatten()\n",
    "        sentiment_vec = np.array(df.iloc[i][[\"sentiment_comp\",\"sentiment_global\"]], dtype=np.float32).flatten()\n",
    "        macro_vec = np.array(df.iloc[i][[\"interest_rate\",\"inflation_rate\",\"gdp_growth\"]], dtype=np.float32).flatten()\n",
    "        vix_value = df[\"vix_index\"].iloc[i]\n",
    "        if pd.isna(vix_value):\n",
    "            vix_value = 0.0\n",
    "        market_vec = np.array([vix_value], dtype=np.float32).flatten()\n",
    "        X_i = np.concatenate([window, sentiment_vec, macro_vec, market_vec])\n",
    "        y_i = np.float32(df.iloc[i+1][\"Close\"])\n",
    "        X.append(X_i)\n",
    "        y.append(y_i)\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "print(\"‚úÖ Data gathering module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5176803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_features(X, y=None):\n",
    "    scaler_X = StandardScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    if y is not None:\n",
    "        scaler_y = StandardScaler()\n",
    "        y_scaled = scaler_y.fit_transform(y.reshape(-1,1))\n",
    "        return X_scaled, y_scaled, scaler_X, scaler_y\n",
    "    return X_scaled, scaler_X\n",
    "\n",
    "print(\"‚úÖ Preprocessing module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Module\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx].view(-1, 1)\n",
    "\n",
    "print(\"‚úÖ Dataset module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Model Architecture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdvancedStockPredictor(nn.Module):\n",
    "    \"\"\"Advanced deep learning model with LSTM, Attention, and Residual connections\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=256, num_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal dependencies\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_dim, hidden_dim, num_layers=num_layers,\n",
    "            batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2, num_heads=8,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dim * 2, dropout) for _ in range(3)\n",
    "        ])\n",
    "        \n",
    "        # Output network\n",
    "        self.output_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout / 2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout / 2),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.LSTM):\n",
    "            for name, param in module.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        attn_out = attn_out.squeeze(1)\n",
    "        out = attn_out\n",
    "        for block in self.residual_blocks:\n",
    "            out = block(out)\n",
    "        prediction = self.output_network(out)\n",
    "        return prediction\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(F.relu(self.block(x) + x))\n",
    "\n",
    "print(\"‚úÖ Model architecture loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Module with Early Stopping\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        elif score < self.best_score - self.min_delta:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "    \n",
    "    def load_best_model(self, model):\n",
    "        if self.best_model_state is not None:\n",
    "            model.load_state_dict(self.best_model_state)\n",
    "\n",
    "def train_deep_learning_model(\n",
    "    X, y, epochs=200, batch_size=64, lr=1e-3,\n",
    "    hidden_dim=256, num_layers=3, dropout=0.3,\n",
    "    train_split=0.7, val_split=0.15, patience=15\n",
    "):\n",
    "    \"\"\"Advanced training with proper splits and regularization\"\"\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training on: {device}\")\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Data splits\n",
    "    n_samples = len(X)\n",
    "    indices = np.arange(n_samples)\n",
    "    test_size = 1.0 - train_split - val_split\n",
    "    train_val_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=42, shuffle=False)\n",
    "    val_size_adjusted = val_split / (train_split + val_split)\n",
    "    train_idx, val_idx = train_test_split(train_val_idx, test_size=val_size_adjusted, random_state=42, shuffle=False)\n",
    "    \n",
    "    print(f\"Data Split:\")\n",
    "    print(f\"  Train: {len(train_idx)} ({len(train_idx)/n_samples*100:.1f}%)\")\n",
    "    print(f\"  Val:   {len(val_idx)} ({len(val_idx)/n_samples*100:.1f}%)\")\n",
    "    print(f\"  Test:  {len(test_idx)} ({len(test_idx)/n_samples*100:.1f}%)\\n\")\n",
    "    \n",
    "    # Scale data\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val, y_val = X[val_idx], y[val_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    \n",
    "    X_train_scaled, y_train_scaled, scaler_X, scaler_y = scale_features(X_train, y_train)\n",
    "    X_val_scaled = scaler_X.transform(X_val)\n",
    "    y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1))\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = StockDataset(X_train_scaled, y_train_scaled)\n",
    "    val_dataset = StockDataset(X_val_scaled, y_val_scaled)\n",
    "    test_dataset = StockDataset(X_test_scaled, y_test_scaled)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = AdvancedStockPredictor(\n",
    "        input_dim=X.shape[1], hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers, dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model Parameters: {total_params:,}\\n\")\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\\n\")\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        train_loss /= len(train_dataset)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "        val_loss /= len(val_dataset)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs} | Train: {train_loss:.6f} | Val: {val_loss:.6f}\")\n",
    "        \n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    early_stopping.load_best_model(model)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Test evaluation\n",
    "    model.eval()\n",
    "    predictions, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            predictions.extend(y_pred.cpu().numpy())\n",
    "            targets.extend(y_batch.numpy())\n",
    "    \n",
    "    predictions = np.array(predictions).flatten()\n",
    "    targets = np.array(targets).flatten()\n",
    "    \n",
    "    pred_orig = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    target_orig = scaler_y.inverse_transform(targets.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    mse = np.mean((pred_orig - target_orig) ** 2)\n",
    "    mae = np.mean(np.abs(pred_orig - target_orig))\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((target_orig - pred_orig) / target_orig)) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Test Set Performance\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"MSE:  {mse:.4f}\")\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return model, history, (scaler_X, scaler_y), {\n",
    "        'predictions': pred_orig,\n",
    "        'targets': target_orig,\n",
    "        'mse': mse, 'mae': mae, 'rmse': rmse, 'mape': mape\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Training module loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a9d62",
   "metadata": {},
   "source": [
    "## üìä Gather and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select stock ticker and gather data\n",
    "TICKER = \"AAPL\"  # Change to any stock ticker\n",
    "DAYS_BACK = 200  # More historical data for better patterns\n",
    "\n",
    "print(f\"Gathering data for {TICKER}...\")\n",
    "X, y = gather_data(TICKER, days_back=DAYS_BACK)\n",
    "\n",
    "print(f\"\\n‚úÖ Data gathered successfully!\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Total samples: {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26ab21",
   "metadata": {},
   "source": [
    "## üöÄ Train Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the advanced model\n",
    "model, history, scalers, test_metrics = train_deep_learning_model(\n",
    "    X, y,\n",
    "    epochs=200,           # Maximum epochs (early stopping will trigger earlier)\n",
    "    batch_size=64,        # Larger batch for more stable gradients\n",
    "    lr=1e-3,              # Learning rate\n",
    "    hidden_dim=256,       # Hidden layer size\n",
    "    num_layers=3,         # LSTM layers\n",
    "    dropout=0.3,          # Dropout rate\n",
    "    train_split=0.7,      # 70% training\n",
    "    val_split=0.15,       # 15% validation\n",
    "    patience=15           # Early stopping patience\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d778f",
   "metadata": {},
   "source": [
    "## üìà Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history['lr'], color='green', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions vs Actual\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(test_metrics['targets'], test_metrics['predictions'], alpha=0.6)\n",
    "plt.plot([test_metrics['targets'].min(), test_metrics['targets'].max()],\n",
    "         [test_metrics['targets'].min(), test_metrics['targets'].max()],\n",
    "         'r--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Price', fontsize=12)\n",
    "plt.ylabel('Predicted Price', fontsize=12)\n",
    "plt.title('Test Set: Predictions vs Actual', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"RMSE: ${test_metrics['rmse']:.2f}\")\n",
    "print(f\"MAE:  ${test_metrics['mae']:.2f}\")\n",
    "print(f\"MAPE: {test_metrics['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5ee85",
   "metadata": {},
   "source": [
    "## üíæ Save Model and Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcef007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('trained_models')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = output_dir / 'advanced_stock_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"‚úÖ Model saved to {model_path}\")\n",
    "\n",
    "# Save scalers\n",
    "scaler_X, scaler_y = scalers\n",
    "with open(output_dir / 'scaler_X.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "with open(output_dir / 'scaler_y.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_y, f)\n",
    "print(f\"‚úÖ Scalers saved\")\n",
    "\n",
    "# Download to local machine\n",
    "from google.colab import files\n",
    "print(\"\\nDownloading files...\")\n",
    "files.download(str(model_path))\n",
    "files.download(str(output_dir / 'scaler_X.pkl'))\n",
    "files.download(str(output_dir / 'scaler_y.pkl'))\n",
    "print(\"\\n‚úÖ All files ready for download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3d814",
   "metadata": {},
   "source": [
    "## üîÆ Make Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_day(model, scalers, ticker, device='cpu'):\n",
    "    \"\"\"Predict next day's closing price\"\"\"\n",
    "    scaler_X, scaler_y = scalers\n",
    "    \n",
    "    # Gather latest data\n",
    "    X_latest, _ = gather_data(ticker, days_back=60)\n",
    "    X_last = X_latest[-1:]\n",
    "    \n",
    "    # Scale and predict\n",
    "    X_scaled = scaler_X.transform(X_last)\n",
    "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_scaled = model(X_tensor)\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled.cpu().numpy())\n",
    "    \n",
    "    return float(y_pred[0, 0])\n",
    "\n",
    "# Example prediction\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "predicted_price = predict_next_day(model, scalers, TICKER, device)\n",
    "print(f\"\\nüîÆ Predicted next day closing price for {TICKER}: ${predicted_price:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1962e0",
   "metadata": {},
   "source": [
    "## üìä Final Summary\n",
    "\n",
    "### Key Improvements Over Simple Model:\n",
    "\n",
    "1. **Advanced Architecture**\n",
    "   - ‚úÖ LSTM layers for temporal pattern recognition\n",
    "   - ‚úÖ Multi-head attention mechanism\n",
    "   - ‚úÖ Residual connections for deeper networks\n",
    "\n",
    "2. **Proper Data Management**\n",
    "   - ‚úÖ 70/15/15 train/val/test split\n",
    "   - ‚úÖ No data leakage (scalers fit on training only)\n",
    "   - ‚úÖ Temporal ordering preserved\n",
    "\n",
    "3. **Regularization Techniques**\n",
    "   - ‚úÖ Dropout layers (0.3 rate)\n",
    "   - ‚úÖ Early stopping (patience=15)\n",
    "   - ‚úÖ L2 weight decay (1e-5)\n",
    "   - ‚úÖ Gradient clipping\n",
    "   - ‚úÖ Layer normalization\n",
    "\n",
    "4. **Training Optimizations**\n",
    "   - ‚úÖ AdamW optimizer\n",
    "   - ‚úÖ Learning rate scheduling\n",
    "   - ‚úÖ GPU acceleration\n",
    "   - ‚úÖ Batch processing\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different hyperparameters\n",
    "- Try different stock tickers\n",
    "- Extend to multi-day predictions\n",
    "- Add more advanced features\n",
    "- Implement ensemble methods"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
